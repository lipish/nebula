# Nebula 部署指南

## 1. 环境要求

| 依赖 | 版本/说明 |
|------|-----------|
| **Rust** | stable（用于编译 nebula 各组件） |
| **etcd** | v3.x，监听 `127.0.0.1:2379` |
| **vLLM** | 支持 `vllm serve` 的版本 |
| **NVIDIA GPU** | 需要足够显存加载目标模型 |
| **CUDA** | 与 vLLM 兼容的版本 |

## 2. 编译

```bash
cargo build --workspace
```

编译产物位于 `target/debug/`（或 `target/release/`），包含：
- `nebula-scheduler`
- `nebula-node`
- `nebula-router`
- `nebula-gateway`
- `nebula-bff`

## 3. 启动 etcd

使用 Docker 快速启动：

```bash
docker run -d --name etcd \
  -p 2379:2379 -p 2380:2380 \
  quay.io/coreos/etcd:v3.5.0 \
  /usr/local/bin/etcd \
  --advertise-client-urls http://0.0.0.0:2379 \
  --listen-client-urls http://0.0.0.0:2379
```

验证健康：

```bash
curl http://127.0.0.1:2379/health
# 预期：{"health":"true","reason":""}
```

## 4. 按顺序启动服务

### 4a. Scheduler — 写入 PlacementPlan

```bash
./target/debug/nebula-scheduler \
  --model-uid qwen2_5_0_5b \
  --model-name Qwen/Qwen2.5-0.5B-Instruct \
  --node-id $(hostname) \
  --engine vllm \
  --port 10814
```

确认输出包含 `placed: key=/placements/qwen2_5_0_5b ...`。

### 4b. Node — 拉起 vLLM 并注册 endpoint

```bash
./target/debug/nebula-node \
  --node-id $(hostname) \
  --etcd-endpoints http://127.0.0.1:2379
```

Node 会：
1. Watch `/placements/` 获取分配给本机的 assignment
2. 启动 vLLM 进程（`vllm serve`）
3. 等待 vLLM ready 后注册 `/endpoints/{model_uid}/{replica_id}`

确认日志包含 `registered endpoint ... base_url=http://127.0.0.1:10814`。

### 4c. 验证 vLLM 就绪

```bash
curl http://127.0.0.1:10814/v1/models
# 预期：200 OK，包含模型名称
```

### 4d. Router — 路由请求到 endpoint

```bash
./target/debug/nebula-router \
  --listen 0.0.0.0:18081 \
  --etcd-endpoints http://127.0.0.1:2379
```

验证健康：

```bash
curl http://127.0.0.1:18081/healthz
# 预期：ok

路由指标：

```bash
curl http://127.0.0.1:18081/metrics
```
```

### 4e. Gateway — 对外提供 OpenAI API

```bash
./target/debug/nebula-gateway \
  --listen 0.0.0.0:8081 \
  --router-url http://127.0.0.1:18081
```

验证健康：

```bash
curl http://127.0.0.1:8081/healthz
# 预期：ok

网关指标：

```bash
curl http://127.0.0.1:8081/metrics
```

#### Control API 鉴权（可选）

为 `/v1/admin/*` 开启鉴权时，设置以下环境变量：

```bash
# token:role 以逗号分隔，role 为 admin/operator/viewer
export NEBULA_AUTH_TOKENS="devtoken:admin,viewtoken:viewer"

# 可选：每分钟每 token 的请求上限
export NEBULA_AUTH_RATE_LIMIT_PER_MINUTE=120
```

请求时携带 token：

```bash
curl -H "Authorization: Bearer devtoken" \
  http://127.0.0.1:8081/v1/admin/cluster/status

查看网关日志（tail 200 行）：

```bash
curl -H "Authorization: Bearer devtoken" \
  "http://127.0.0.1:8081/v1/admin/logs?lines=200"
```

访问 Web Console（MVP）：

```bash
open http://127.0.0.1:8081/v1/admin/ui
```
```
```

## 5. BFF 与 xtrace 鉴权模式（安装部署建议）

BFF 提供 `/api/v2/*`、`/api/audit-logs`、`/api/observe/*` 等管理与观测接口，访问 xtrace 时支持两种模式：

- `XTRACE_AUTH_MODE=internal`：内网信任模式，不向 xtrace 发送 Bearer token（开发环境推荐）
- `XTRACE_AUTH_MODE=service`：服务鉴权模式，必须配置 `XTRACE_TOKEN`（生产环境推荐）

### 开发环境推荐（默认）

```bash
export XTRACE_URL=http://127.0.0.1:8742
export XTRACE_AUTH_MODE=internal

./target/debug/nebula-bff \
  --listen-addr 0.0.0.0:18090 \
  --etcd-endpoint http://127.0.0.1:2379 \
  --router-url http://127.0.0.1:18081
```

### 生产环境推荐

```bash
export XTRACE_URL=http://xtrace:8742
export XTRACE_AUTH_MODE=service
export XTRACE_TOKEN=<your-internal-service-token>

./target/debug/nebula-bff \
  --listen-addr 0.0.0.0:18090 \
  --etcd-endpoint http://127.0.0.1:2379 \
  --router-url http://127.0.0.1:18081
```

当 `XTRACE_AUTH_MODE=service` 且 `XTRACE_TOKEN` 为空时，BFF 会返回配置错误，避免误以为是权限问题。

## 6. 快速验证

### 非流式 Chat

```bash
curl -s http://127.0.0.1:8081/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-0.5B-Instruct",
    "messages": [{"role": "user", "content": "Hello"}]
  }' | python3 -m json.tool
```

### 流式 Chat

```bash
curl -N http://127.0.0.1:8081/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-0.5B-Instruct",
    "messages": [{"role": "user", "content": "Hello"}],
    "stream": true
  }'
```

### 流式 Responses

```bash
curl -N http://127.0.0.1:8081/v1/responses \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-0.5B-Instruct",
    "input": "Hello",
    "stream": true
  }'
```

## 7. 端口汇总

| 服务 | 默认端口 | 说明 |
|------|----------|------|
| etcd | 2379 | 元数据存储 |
| vLLM | 10814 | 模型推理（由 node 自动启动） |
| Router | 18081 | 请求路由 |
| BFF | 18090 | 管理与 v2 API（审计/观测/模型管理） |
| Gateway | 8081 | 对外 API 入口 |
| xtrace | 8742 | 审计与观测后端 |

## 8. 常见问题

### 端口被占用

启动前检查端口是否空闲：

```bash
ss -tlnp | grep -E '(8081|18090|18081|10814|8742)'
```

如有残留进程，先清理：

```bash
pkill -f 'nebula-router'
pkill -f 'nebula-gateway'
pkill -f 'nebula-node'
pkill -f 'vllm serve'
```

### vLLM 孤儿进程

`pkill -f 'vllm serve'` 可能无法杀掉 vLLM 的子进程（如 `VLLM::EngineCore`）。需要：

```bash
# 查找残留的 vLLM 相关进程
nvidia-smi  # 查看 GPU 占用
ps aux | grep -i vllm

# 手动 kill 孤儿进程
kill <pid>
# 如果 TERM 无效
kill -9 <pid>
```

### Router 启动无输出

Router 如果绑定端口失败会静默退出。启动时建议加 `RUST_BACKTRACE=1`：

```bash
RUST_BACKTRACE=1 ./target/debug/nebula-router --listen 0.0.0.0:18081 --etcd-endpoints http://127.0.0.1:2379
```

---

## 9. Docker Compose（控制面）

提供基础控制面编排（含 etcd/gateway/router/scheduler/bff，不含 GPU 的 nebula-node / vLLM）。

```bash
docker compose up -d --build
```

开发环境可直接：

```bash
XTRACE_AUTH_MODE=internal docker compose up -d --build
```

生产建议：

```bash
XTRACE_AUTH_MODE=service \
XTRACE_TOKEN=<internal-service-token> \
XTRACE_URL=<xtrace-endpoint> \
docker compose up -d --build
```

如需在 compose 内同时启动 xtrace（可选）：

```bash
XTRACE_URL=http://xtrace:8742 \
XTRACE_AUTH_MODE=service \
XTRACE_TOKEN=<internal-service-token> \
docker compose --profile observe up -d --build
```

可选环境变量：
- `XTRACE_IMAGE`：xtrace 镜像地址（默认 `ghcr.io/lipish/xtrace:latest`）
- `XTRACE_DATABASE_URL`：xtrace 数据库连接串

默认使用 `NEBULA_AUTH_TOKENS="devtoken:admin,viewtoken:viewer"`，可在 `docker-compose.yml` 中调整。
